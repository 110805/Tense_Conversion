{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOa0H228Os9oUnV32YHU3Ya",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/110805/Tense_Conversion/blob/master/CVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzU3drMAYuJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "bb5cf818-ead5-4be7-909e-41dae078f0be"
      },
      "source": [
        "!git clone https://github.com/110805/Tense_Conversion.git\n",
        "%cd Tense_Conversion/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Tense_Conversion'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n",
            "/content/Tense_Conversion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1hhLchfZCw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from os import system\n",
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
        "\n",
        "\"\"\"========================================================================================\n",
        "The sample.py includes the following template functions:\n",
        "\n",
        "1. Encoder, decoder\n",
        "2. Training function\n",
        "3. BLEU-4 score function\n",
        "4. Gaussian score function\n",
        "\n",
        "You have to modify them to complete the lab.\n",
        "In addition, there are still other functions that you have to \n",
        "implement by yourself.\n",
        "\n",
        "1. The reparameterization trick\n",
        "2. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)\n",
        "3. Output your results (BLEU-4 score, conversion words, Gaussian score, generation words)\n",
        "4. Plot loss/score\n",
        "5. Load/save weights\n",
        "\n",
        "There are some useful tips listed in the lab assignment.\n",
        "You should check them before starting your lab.\n",
        "========================================================================================\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "#----------Hyper Parameters----------#\n",
        "hidden_size = 256\n",
        "latent_size = 32\n",
        "#The number of vocabulary\n",
        "vocab_size = 28\n",
        "tense_size = 4\n",
        "teacher_forcing_ratio = 0.7\n",
        "#empty_input_ratio = 0.1\n",
        "KLD_weight = 0.0\n",
        "LR = 0.05\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "#Example inputs of compute_bleu\n",
        "################################\n",
        "#The target word\n",
        "reference = 'accessed'\n",
        "#The word generated by your model\n",
        "output = 'access'\n",
        "\n",
        "#compute BLEU-4 score\n",
        "def compute_bleu(output, reference):\n",
        "    cc = SmoothingFunction()\n",
        "    if len(reference) == 3:\n",
        "        weights = (0.33,0.33,0.33)\n",
        "    else:\n",
        "        weights = (0.25,0.25,0.25,0.25)\n",
        "    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)\n",
        "\n",
        "\"\"\"============================================================================\n",
        "example input of Gaussian_score\n",
        "\n",
        "words = [['consult', 'consults', 'consulting', 'consulted'],\n",
        "['plead', 'pleads', 'pleading', 'pleaded'],\n",
        "['explain', 'explains', 'explaining', 'explained'],\n",
        "['amuse', 'amuses', 'amusing', 'amused'], ....]\n",
        "\n",
        "the order should be : simple present, third person, present progressive, past\n",
        "============================================================================\"\"\"\n",
        "\n",
        "def Gaussian_score(words):\n",
        "    words_list = []\n",
        "    score = 0\n",
        "    yourpath = ''#should be your directory of train.txt\n",
        "    with open(yourpath,'r') as fp:\n",
        "        for line in fp:\n",
        "            word = line.split(' ')\n",
        "            word[3] = word[3].strip('\\n')\n",
        "            words_list.extend([word])\n",
        "        for t in words:\n",
        "            for i in words_list:\n",
        "                if t == i:\n",
        "                    score += 1\n",
        "    return score/len(words)\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def get_train_pair(i, Data):\n",
        "    input_tensor = []\n",
        "    target_tensor = []\n",
        "    \n",
        "    for char in Data[i]:\n",
        "        input_tensor.append(ord(char)-95)\n",
        "        target_tensor.append(ord(char)-95)\n",
        "\n",
        "    target_tensor.append(EOS_token)\n",
        "    return (torch.tensor(input_tensor, dtype=torch.long).view(-1, 1), torch.tensor(target_tensor, dtype=torch.long).view(-1, 1))\n",
        "\n",
        "#Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size + tense_size)\n",
        "        self.hidden2mean = nn.Linear(hidden_size + tense_size, latent_size)\n",
        "        self.hidden2var = nn.Linear(hidden_size + tense_size, latent_size)\n",
        "        #self.cell2mean = nn.Linear(hidden_size + tense_size, latent_size)\n",
        "        #self.cell2var = nn.Linear(hidden_size + tense_size, latent_size)\n",
        "        self.latent2hidden = nn.Linear(latent_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(len(input), 1, -1)\n",
        "        output = embedded\n",
        "        output, (hidden, cell) = self.lstm(output, hidden)\n",
        "\n",
        "        mean = self.hidden2mean(hidden)\n",
        "        logvar = self.hidden2var(hidden)\n",
        "        #c_mean = self.cell2mean(hidden[1])\n",
        "        #c_logvar = self.cell2var(hidden[1])\n",
        "        z_h = self.reparameterization(mean, logvar)\n",
        "        #z_c = self.reparameterization(c_mean, c_logvar)\n",
        "        hidden = self.latent2hidden(z_h)\n",
        "\n",
        "        #return output, (z_h, z_c)\n",
        "        return output, hidden , cell, self.KL_loss(mean, logvar)\n",
        "\n",
        "    def reparameterization(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn(1, 1, latent_size) # sample a gaussain noise from N(0, I)\n",
        "        z = mean + std*eps\n",
        "        return z\n",
        "\n",
        "    def KL_loss(self, mean, logvar):\n",
        "        # KLD = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        return -0.5 * torch.sum(1 + logvar - mean*mean - torch.exp(logvar))\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "#Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size + tense_size)\n",
        "        self.out = nn.Linear(hidden_size + tense_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.out(output[0])\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "def evaluate(encoder, decoder, input_string, tense, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = []\n",
        "        for input_char in input_string:\n",
        "            input_tensor.append(ord(input_char)-95)\n",
        "\n",
        "        input_tensor = torch.tensor(input_tensor, dtype=torch.long).view(-1, 1)\n",
        "        input_tensor = input_tensor.to(device)\n",
        "\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initHidden()\n",
        "        c = torch.eye(4)[tense[0]].view(1, 1, -1)\n",
        "        encoder_hidden = torch.cat((encoder_hidden, c), 2)\n",
        "        encoder_cell = torch.cat((encoder_cell, torch.zeros(1,1,4)), 2)\n",
        "        encoder_output, encoder_hidden, encoder_cell, KL_loss = encoder(input_tensor, (encoder_hidden, encoder_cell))\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        c = torch.eye(4)[tense[1]].view(1, 1, -1)\n",
        "        encoder_hidden = torch.cat((encoder_hidden, c), 2)\n",
        "        decoder_hidden = (encoder_hidden, encoder_cell)\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(chr(topi.item()+95))\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        pred = ''\n",
        "        for i in range(len(decoded_words)):\n",
        "            pred += decoded_words[i]\n",
        "\n",
        "        return pred\n",
        "\n",
        "def evalTestdata(encoder, decoder):\n",
        "    score = 0\n",
        "    with open('test.txt', 'r') as f:\n",
        "        all_lines = f.readlines()\n",
        "\n",
        "    Input = []\n",
        "    Target = []\n",
        "    tenses = [[0,3], [0,2], [0,1], [0,1], [3,1], [0,2], [3,0], [2,0], [2,3], [2,1]]\n",
        "    for line in all_lines:\n",
        "        if line[-1] == '\\n':\n",
        "            line = line[:-1]\n",
        "\n",
        "        words = line.split(' ')\n",
        "        Input.append(words[0])\n",
        "        Target.append(words[1])\n",
        "    \n",
        "    for i in range(len(Input)):\n",
        "        output = evaluate(encoder, decoder, Input[i], tenses[i])\n",
        "        #print('input: {}'.format(Input[i]))\n",
        "        #print('target: {}'.format(Target[i]))\n",
        "        #print('pred: {}'.format(output))\n",
        "        \n",
        "        if len(output) != 0:\n",
        "            score += compute_bleu(output, Target[i])\n",
        "        else:\n",
        "            score += compute_bleu('', Target[i]) # predict empty string\n",
        "        \n",
        "        #print('--------------------')\n",
        "    #print('BLEU-4 score:{}'.format(score/50))\n",
        "    return score/len(Input)\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, tense, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initHidden()\n",
        "    c = torch.eye(4)[tense].view(1, 1, -1)\n",
        "    encoder_hidden = torch.cat((encoder_hidden, c), 2)\n",
        "    encoder_cell = torch.cat((encoder_cell, torch.zeros(1,1,4)), 2)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "    #----------sequence to sequence part for encoder----------#\n",
        "    encoder_output, encoder_hidden, encoder_cell, KL_loss = encoder(input_tensor, (encoder_hidden, encoder_cell))\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    encoder_hidden = torch.cat((encoder_hidden, c), 2)\n",
        "    decoder_hidden = (encoder_hidden, encoder_cell)\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    #----------sequence to sequence part for decoder----------#\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    \n",
        "    Loss = loss + KL_loss\n",
        "    Loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length, KL_loss\n",
        "\n",
        "def trainIters(encoder, decoder, n_epochs, learning_rate=LR):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    plot_KL_losses = []\n",
        "    BLEU_scores = []\n",
        "    epoch_loss = 0\n",
        "    epoch_KL_loss = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    with open('train.txt', 'r') as f:\n",
        "        all_lines = f.readlines()\n",
        "\n",
        "    Data = []\n",
        "    for line in all_lines:\n",
        "        if line[-1] == '\\n':\n",
        "            line = line[:-1]\n",
        "\n",
        "        words = line.split(' ')\n",
        "        for word in words:\n",
        "            Data.append(word)\n",
        "\n",
        "    training_pairs = [get_train_pair(i, Data) for i in range(len(Data))]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        for iters in range(len(Data)):\n",
        "            training_pair = training_pairs[iters]\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "\n",
        "            loss, KL_loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion, iters % 4)\n",
        "        \n",
        "            epoch_loss += loss\n",
        "            epoch_KL_loss += KL_loss\n",
        "\n",
        "        epoch_loss_avg = epoch_loss / len(Data)\n",
        "        epoch_KL_loss_avg = epoch_KL_loss / len(Data)\n",
        "        plot_losses.append(epoch_loss_avg)\n",
        "        plot_KL_losses.append(epoch_KL_loss_avg)\n",
        "        epoch_loss = 0\n",
        "        epoch_KL_loss = 0\n",
        "        bleu_score = evalTestdata(encoder, decoder)\n",
        "        BLEU_scores.append(bleu_score)\n",
        "        print('%s (%d %d%%) %.4f %.4f %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, epoch_loss_avg, epoch_KL_loss_avg, bleu_score))\n",
        "        \n",
        "        torch.save(encoder.state_dict(), 'encoder.pkl')\n",
        "        torch.save(decoder.state_dict(), 'decoder.pkl')\n",
        "        \n",
        "    plt.figure(1)\n",
        "    plt.plot(range(n_epochs), plot_losses)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('CrossEntropyLoss')\n",
        "    plt.savefig('CELoss')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(range(n_epochs), plot_KL_losses)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('KL_Loss')\n",
        "    plt.savefig('KLLoss')\n",
        "\n",
        "    plt.figure(3)\n",
        "    plt.plot(range(n_epochs), BLEU_scores)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('BLEU_scores')\n",
        "    plt.savefig('BLEU_scores')\n",
        "\t\n",
        "encoder1 = EncoderRNN(vocab_size, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, vocab_size).to(device)\n",
        "trainIters(encoder1, decoder1, 60)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}